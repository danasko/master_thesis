\chapter{Implementation}\label{chap:implementation}
This chapter is dedicated to the description of the implementation details of the particular methods and neural models. We will introduce the architecture of the models, as well as the data pre-processing, training procedures and optimization parameters used.

\section{Deep Depth Pose model}
In this section, we are going to describe our implementation of the DDP model (proposed in \cite{Marin18jvcir}) in Keras framework. The follow-up additional modifications of the model are presented at the end of the section.\par
\vspace{5mm}
\noindent
The DDP model is based on the idea of linearly combining the predefined prototype poses to obtain the resulting pose estimation. The set of prototype poses is produced by clustering the training dataset into $K$ clusters, $K$ being a hyperparameter of the network. The output of the model represents $K$ weights, each corresponding to one of the prototype poses. The final estimated pose is then obtained as a sum of the weighted prototypes.


\subsection{Model structure}

The architecure of the DDP model itself is relatively simple. The model is actually not very deep, it consists of five convolutional blocks, each followed by ReLU activation. First three blocks also contain a pooling layer, as indicated in Figure \ref{fig:ddp}. After the convolutional blocks, there are three fully-connected layers. The number of neurons in the output layer is a hyperparameter set to the number of prototype poses clustered from the dataset. The input to the model is a one-channel depth map of size 100 x 100 pixels.\par

\vspace{5mm}
\begin{figure}[H]
\begin{center}
  \includegraphics[width=\textwidth]{images/implementation/ddp.PNG}
  \caption[Deep Depth Pose model architecture \cite{Marin18jvcir}.]{Deep Depth Pose model architecture \cite{Marin18jvcir} (\textit{P} stands for pooling layer, \textit{Dr} indicates dropout, $K$ is the number of prototype poses).}
  \label{fig:ddp}
\end{center}
\end{figure}


\subsection{Loss function}

The loss function used during training the model is defined as huber loss with a regularization term, as described in Equation \ref{eq:huberloss}, where $\mathcal{L}_R$ is the (residual) huber loss.\par

\begin{equation}
\mathcal{L}_{DDP}(g(\mathbf{D}, \theta), \mathbf{C}, \mathbf{p}, \alpha) = (1 - \alpha) \cdot \mathcal{L}_R(\mathbf{C} \times g(\mathbf{D}, \theta), \mathbf{p}) + \alpha \cdot \|g(\mathbf{D}, \theta)\|_1 \label{eq:huberloss}
\end{equation} 

\noindent In the equation above, $g(\mathbf{D}, \theta)$ represents a non-linear function computed by the neural model on the input depth map $\mathbf{D}$ with trainable parameters $\theta$, returning a column vector of length $K$ (number of clustered prototype poses), $\mathbf{C}$ is a matrix with $K$ columns containing prototype poses, $\mathbf{p}$ stands for the vectorized ground truth pose,  and $\alpha$ is a regularization coefficient  – a hyperparameter to control the magnitude of the resulting weights of the prototypes.

\subsection{Initialization}

The values of the model's hyperparameters from the original DDP paper were chosen using the 'Mann-Whitney U-test' \cite{mann1947}, thus determining, whether the median difference between pairs of configurations are statistically significant. The value of the regularization coefficient was fixed to $\alpha = 0.08$ for the ITOP dataset, and $\alpha = 0.01$ for the UBC3V dataset. The number of prototypes was set to $K=70$ and $K=100$ regarding the ITOP and the UBC3V dataset respectively.\par
\vspace{5mm}
\noindent
The model learns using the Adam optimizer with the learning rate set to $10^{-3}$, which is progressively decreasing during the training.  The weights in the model are initialized using the Xavier normal initializer, that is randomly chosen from the normal distribution with zero mean and a standard deviation proportional to the filter size. The biases are initially set to zero. The size of batches is set to $b=64$.

\subsection{Data pre-processing}

In the pre-processing stage, the depth maps are re-sized to match the model input dimensions. First, the image is cropped along the larger dimension on both sides to fit the square, assuming the subject is located near the center of the image. Then, the image is re-sized to $100\times100$ px. Afterwards, the depth values are normalized to range [0,1]. Also, the input depth images are normalized globally by subtracting the mean image of the training set from each depth sample. The ground truth poses are being normalized as well – to zero mean and one standard deviation.\par
\vspace{5mm}
\noindent 
Concerning the datasets used for evaluation, UBC3V already comes with the split into the train, validation and test set. The ITOP dataset is originally divided into the train and test set only, thus the validation set was acquired by random sampling of the train data. The ratio was set to 80/20, meaning the data used for validation are representing 20\% of the train set. During the experiments, only the hard-pose part of UBC3V, and the side-view part (for single-view approach) of the ITOP dataset were used.

%\subsection{Train phase}
%
%\subsection{Test phase}

\subsection{Modifications}
This section introduces several modifications we have made to the proposed DDP model, in order to lower the mean error of the predictions. Since the experience of the researchers made us believe that the implementation of the same model in different framework often leads to different results, we diverted from the original architecture of the model, as well as the initial optimization parameters.\par
\vspace{5mm}

\noindent
However, still inspired by the DDP model, we have built a slightly deeper, more complex model, in an attempt to improve the final estimations, assuming the original architecture was not able to fully retain the complexity of the specified task. We used both ITOP and UBC3V datasets again, for the evaluation of the modified model.
The architecture of the modified DDP network is shown in Figure \ref{fig:mddp}. As shown in the figure, we significantly increased the amount of dropout layers, as the model tended to overfit the training data. Moreover, two additional convolutional layers were stacked in the model architecture, to enhance the nonlinearity of the network (introduced by the ReLU activations used after convolutions).\par
\vspace{5mm}

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\textwidth]{images/implementation/mddp.png}
  \caption[Our modification of the original DDP model architecture.]{Our modification of the original DDP model architecture\break (\textit{K} stands for the number of prototype poses).}
  \label{fig:mddp}
\end{center}
\end{figure}


\noindent Besides the structure of the model, some of the hyperparameters were slightly modified as well, mainly due to switching to another deep learning framework, where the back-end procedures differ, which may be resulting in a convergence to different outputs. Based on the experiments, the regularization parameter was fixed to $\alpha = 0.1$ for both benchmark datasets. The residual huber loss has been replaced by an absolute error, and the L2 norm was used in the regularization term instead of the L1 norm. Even though in the original paper, the authors indicate the L1 norm is more robust to outliers, our tests in the utilized framework led to a conclusion, that using the L2 norm as a regularization term in the loss function was more helpful to the loss minimization. Also, the initial learning rate was reduced to $10^{-4}$, due to an occasional undesirable divergent behavior of the model loss.\par
\vspace{5mm}
\noindent Regarding the test phase, the experiments on the ITOP dataset have shown that even though the test set seems sufficiently representative (that is, retains the approximate distribution of the training set), the error on the test data is by far worse than the error on previously unseen data obtained by random sampling of the training set (used as a validation set). This is probably caused by the fact, that the input data is formed by a sequence of video frames, thus many subsequent samples captures almost the same content, with only a minimal shift between the frames. As a result, a lot of the validation samples are nearly the same as the ones used for training.\par
\vspace{5mm}
\noindent
The final results of both the original DDP and modified DDP model, evaluated on the test set of ITOP and UBC3V datasets, are reported and discussed in Chapter 7.


\section{Point-Based Pose Estimation model}

As another part of our study, we chose to examine the Point-Base Pose Estimation (PBPE) model \cite{Ali19}. Again, we re-implement the method in Keras framework, though the original implementation was in pure Tensorflow. However, note that the original code was not published prior to our implementation.\par
\vspace{5mm}
\noindent % TODO este popisat zakladnu myslienku modelu
The model is heavily inspired by the implementation of PointNet \cite{DBLP:journals/corr/QiSMG16}, which was built for the task of object classification and semantic segmentation. Basically, the architecture has been modified to fit the task of human pose estimation, making use of the auxiliary sub-network contributing to the global model loss.


\subsection{Model structure}
The architecture of the proposed model consists of two branches (or sub-networks). The auxiliary sub-network is included to compute the body part segmentation on the fly, while the main network regresses the joint locations. The basic idea behind the structure of the model is the aggregation of both local and global features of the input point cloud in the auxiliary part-segmentation network. Without the sub-network, almost all of the local context would be lost because of the max-pooling operation in the intermediate layers. The incorporation of the local features helps the network understand the relationships among particular local regions of the human body. The whole architecture of the stated model is shown in Figure \ref{fig:PBPE}.\par
\vspace{5mm}

\begin{figure}[H]
\begin{center}
  \includegraphics[height=200px]{images/implementation/pbpe2.PNG}
  \caption{The Point-Based Pose Estimation model architecture \cite{Ali19}.}
  \label{fig:PBPE}
\end{center}
\end{figure}

\noindent
As already mentioned, the unorganized and irregular depth input represented by a point cloud is a key factor in this architecture. For this very reason, the engagement of the classic convolution operation would be meaningless, since the point cloud has provides no explicit spatial information about the neighboring points. In this case, pseudo-convolutions with a kernel size of $1\times1$ are employed. The function of such pseudo-convolution layers is reducing (or expanding) the dimensionality in the filter space. A stack of the $1 \times 1$ convolution layers, each followed by batch normalization and ReLU activation, is illustrated in Figure \ref{fig:PBPE} as a shared multilayer perceptron (shared MLP). The numbers in the parentheses indicate the number of filters in particular layers.\par
\vspace{5mm}
\noindent After the shared multilayer perceptron, the regression branch consists of two fully-connected layers and an output layer. Each dense layer is also followed by a batch normalization layer and ReLU activation. The input of the model is of shape $(b \times p \times 3)$, where $b$ represents the batch size, and $p$ is the number of points in the input point cloud, each located by three coordinates. The output shape of both sub-networks depends on the number of joints in the skeleton structure. The number of joints, as well as the number of points the point clouds are containing, is a hyper-parameter.\par
\vspace{5mm}
\noindent The input passed to the auxiliary segmentation sub-network is formed by concatenating the local and the global features obtained in the regression branch. The local features contains the outputs of pseudo-convolutions before the max-pooling is applied. The global features are acquired as the result of max-pooling across all points in the point cloud. The auxiliary sub-network outputs per-point labels segmenting the input point cloud into particular body parts.\par
\vspace{5mm}
\noindent One of the conveniences of the PBPE model is the optional omission of the auxiliary branch at test time. Since we are focusing strictly on the task of pose estimation, the body part segmentation is only relevant for us in a context of training (and contributing to the model loss). Hence, we can prevent the data from being passed through the segmentation branch at the inference time, saving computational cost and time.

\subsection{Loss function}
As a loss function of the regression branch, the model uses a simple mean squared error. Regarding the segmentation branch, the categorical cross-entropy is being used, since it is essentially a classfication task.\par
\vspace{5mm}
\noindent
According to our experiments, without the segmentation sub-network, the model tends to slightly overfit the train data. The loss of the auxiliary segmentation branch is contributing to the global loss to help the model generalize better on previously unseen data. Originally, the weight of the contribution was set to $w_a=0.1$. The default weight of the global loss is $w_g = 0.9$. 


\subsection{Initialization} 
The weights inside the model are sampled from the normal distribution with zero mean and standard deviation relative to filter size, using the Xavier initializer. Following the PBPE proposal, the initial learning rate is set to $10^{-3}$, and is reduced exponentially with a decay rate $d = 0.5$ in each epoch. Also, the learning rate is clipped at the value of $10^-5$, in order to prevent the model training from getting stuck in local minima. The model is trained using the Adam optimizer. In each step, the model processes a batch of size $b=32$.\par
\vspace{5mm}
\noindent Another essential hyper-parameter is the predefined number of points in the input point clouds. The value is set to $p=2048$, which seems to be an adequate amount of points to cover the whole human pose with enough complexity. Therefore, every point cloud needs to be sub-sampled to this number of points, prior to feeding it to the network. The number of joints in the skeleton representation, defining the output shape of the both model branches, should be adjusted according to the dataset the network is processing at the moment.

\subsection{Data pre-processing}
% TODO marge datasets into one subsection ? - or at least the shared part of preprocessing (subsampling, segmentation) + pcl body part automatic annotation + normalizacia dat pri vsetkych datasetoch

%\subsubsection{ITOP}

% ..TODO text + ref. Figure \ref{fig:itop_seg} 

%\begin{figure}[H]
%\begin{center}
%  \includegraphics[height=140px]{images/implementation/itop_seg.png}
%  \caption[Sample point cloud from ITOP-front dataset \cite{haque2016viewpoint}.]{ Sample point cloud from ITOP-front dataset \cite{haque2016viewpoint}, left – before, right – after segmentation.}
%  \label{fig:itop_seg}
%\end{center}
%\end{figure}

%\subsubsection{UBC3V}

Depending on the single or multi-view approach, the input point clouds are treated each as a single sample, or the different viewpoints of the same frame are merged into a single point cloud. Based on the existing studies, inference from multi-view input data is expected to give more accurate results, however the single-view based estimation is far more useful in most of the real-time applications, since there is no need for camera synchronization. Therefore, we have carried out experiments inferring from multi-view, as well as single-view input data.\par% TODO viac o tom..  plusy/minusy, testy na oboch sposoboch
% In case of the UBC3V dataset, samples from three cameras are merged together. % TODO vysvetlit preco, .. obsahuje pomerne zlozite pozy
\par
\vspace{5mm}
\noindent
As the next pre-processing step, the point clouds were sub-sampled to the specified number of points ($p = 2048$) using the farthest point sampling technique. The input point clouds and the corresponding joint locations were normalized to the range $ [-1, 1]$ using the minimum and maximum values of the whole training set. Concerning the validation and test data, there were two options, how to normalize them – either using the scaling parameters of the training set, or scale the validation and test set to the specified range with its own parameters. However, since the model is designed to work in a scenario, where the test data are fed into the network one-by-one, the only possible option is scaling it using the known parameters of the train set.\par
%\vspace{5mm}
%\noindent
%In order to increase the number of joints in the skeletal model, in some of the experiments, the interpolation of the joint locations was performed as another part of the pre-processing pipeline.  \textcolor{red}{TODO compare 29 vs 35 joints in Table ..} It is interesting to observe how the number of joints in the skeleton (and the complexity of the model) influences the pose estimation results.

%\subsubsection{MHAD}
% TODO rozpisat podrobnejsie ?
%The MHAD dataset provides each frame captured from two camera viewpoints, one situated in the front, the other one in the back of the subject.

\vspace{5mm}

\begin{figure}[H]
\begin{center}
  \includegraphics[width=\textwidth]{images/implementation/mhad.png}
  \caption[Data pre-processing shown on a sample point cloud from MHAD dataset \cite{Vidal:2013:BMC:2478277.2478412}.]{ Data pre-processing shown on a sample point cloud from MHAD dataset \cite{Vidal:2013:BMC:2478277.2478412} a) before segmentation, b) after removing the background and floor, c) after final clustering.}
  \label{fig:mhad}
\end{center}
\end{figure}

\noindent
Since the used datasets (except for the UBC3V) captures real-world data, the raw point clouds contain a lot of noise. Therefore, we consider suitable preprocessing as a very important step. First, the background wall and the floor are removed by MSAC plane fitting (which is a variation of RANSAC algorithm). Then, the segmentation of the point cloud into clusters based on Euclidean distance is performed, and the biggest cluster is considered as the desired subject. Figure \ref{fig:mhad} depicts a sample from MHAD dataset, before and after applying the segmentation pipeline.% TODO spoliehame sa pri clusterovani na to, ze ludsky subjekt je v obraze najvacsi, teda nenachadzaju sa v scene ine objekty, vacsie ako actor.
\par

\vspace{5mm}
\noindent
The MHAD dataset does not provide any partition into the train and test data. Thus, the test data was selected as randomly sampled 25\% of the whole dataset during the experiments, while the rest was marked as the train set. % spomenut aj leave-one-out subject partition
As already pointed out in the original proposal of the PBPE model, the skeleton representation in the MHAD dataset is very detailed, yielding a total of 35 skeletal joints. Though this provides the ability of estimating the pose with higher complexity, some of the joints have the locations in the skeleton representation so close to each other, they may be considered redundant when estimating a full-body pose. Among these joints are two pairs of nodes located at toe tips and one additional pair of nodes representing the fingertips.
\textcolor{red}{TODO compare 29 vs 35 joints in Table - in Results chapter} As shown in Table ..., it is interesting to observe how the number of joints in the skeleton (and the complexity of the model) influences the pose estimation results.\par
\vspace{5mm}
\noindent 
As the real datasets do not comprise the partition into the particular body regions, the automated technique for the annotation has been proposed in the original implementation of PBPE model. Each skeletal joint is assigned one body region. Using the automatic annotation, each point of the point cloud is associated with the closest skeletal joint based on Euclidean distance. The example of the annotated point cloud is shown in Figure \ref{fig:reg_annotation}.\par 

\vspace{5mm}
\begin{figure}[H]
\begin{center}
  \includegraphics[height=200px]{images/implementation/region_annotation.png}
  \caption[The automatic body region annotation on real data.]{The automatic body region annotation on a sample real data point cloud.}
  \label{fig:reg_annotation}
\end{center}
\end{figure}

\subsection{Modifications}
For the sake of achieving as accurate pose estimation as possible, we made a number of modifications to the model implementation, to use the full potential of the selected framework. In the architecture of our modified PBPE model, we put even greater emphasis on the local feature extraction, chaining additional pseudo-convolutional layers into the shared multilayer perceptron before max-pooling. Aside from that, we also left out the dropout in the regression branch, since with a help from the auxiliary branch, the model had not suffer from overfitting. Moreover, we skipped all the batch normalization layers in the main branch, since the performance improved when excluding them.\par
\vspace{5mm}
\noindent
The loss function has been replaced by mean absolute error, as it helped the model to converge faster in our case. The hyperparameters of the model mostly preserve the same values as in the original paper, except for the learning rate and loss weights. Given by our observations, we fine-tuned the weight parameter of the global loss to $w_g=1$, and decreased the weight of the segmentation branch to $w_a = 0.01$. We set the initial learning rate to $5^{-4}$, while reducing it with an exponential decay rate of $d = 0.2$ in each epoch. Again, the differences between the performance with the particular hyper-parameter values might be caused by switching to another deep learning framework. 


\section{Four-channel Pose Estimation} % TODO

This section is dedicated to the implementation of a novel approach we present as a part of the thesis contribution. The core of the proposed method is a two-stage pipeline, consisting of a body segmentation network and a regression network. The two subsequent stages take a point cloud as an input and produce skeletal joint coordinates as a result.\par
\vspace{5mm}
\noindent The aim of the model is to embrace both local and global features of the input point cloud, and thus increase the accuracy of the final estimation of the human pose. Also, the model makes use of the residual connections in-between the layers to help the gradient flow, and to avoid the degradation problem as the network depth increases.


\subsection{Model structure}
In both networks of the pipeline, the input point clouds are being processed in the same fashion as in the PBPE model, i.e. by employing the $1 \times 1$ convolutional layers. The architecture of the whole pipeline is described in Figure \ref{fig:4chan_pipeline}.\par
\vspace{5mm}
\noindent First, an input point cloud of the captured subject is passed to the body segmentation network, assuming the input is already sub-sampled to the specified number of points. The segmentation model contains two shared multilayer perceptron blocks, which are – compared to those used in the PBPE model – upgraded by residual skip connections added between the convolution operations. The outcome of the first block represents the local features extracted from the input. Next, much like in the PBPE model, a max-pooling across all points is applied and the obtained global features are concatenated with the local features. Subsequently, the gathered features are fed into the second block of pseudo-convolutions. Here, apart from the shortcut connections, a dropout layer is included between each convolution. The segmentation network outputs a point-wise classification of the point cloud into the corresponding body regions.\par
\vspace{5mm}
\vspace{5mm}
\begin{figure}[H]
\begin{center}
  \includegraphics[width=\textwidth]{images/implementation/4chan_pipeline_fixed.png}
  \caption[The Four-channel Pose Estimation pipeline.]{ The Four-channel Pose Estimation pipeline: First, point clouds are segmented into body regions in the segmentation network (up), then the input point clouds are concatenated with the predicted body regions as a fourth channel, and fed into the regression network (down).}
  \label{fig:4chan_pipeline}
\end{center}
\end{figure}

\noindent The produced partition into the body regions is concatenated with the original point cloud, hence forming a four-channel input. Such data is passed through two shared multilayer perceptron blocks. After the first one, the outputs of all three convolution layers forming the block are summed up in a residual connection, before entering the second block. Again, the ReLU activation function is used after each layer inside both blocks. As the following layer, a global average pooling is incorporated instead of the commonly used flatten layer, to spatially average across each feature map, and therefore to avoid having majority of the model parameters concentrated in the first fully-connected layer. As indicated, next in the network architecture are two dense layers with 512 and 256 filters, respectively. The output layer is producing 3D coordinates of skeletal joints, hence its dimension is relative to the number of joints in the body skeleton.

% TODO in results ? - the 4chan model proved to benefit from the redundancy in the network inputs, being able to absorb the local and global context at once.

\subsection{Loss function}
In the first (segmentation) network, the categorical cross-entropy is employed as a loss function, to measure the accuracy of the body part classification. In the case of the regression network, the mean absolute error between the predicted locations and the ground truth labels of all skeletal joints is used to determine the model loss.

\subsection{Initialization}
Both networks are trained using the Adam optimizer with the initial learning rate equal to $10^{-3}$, and an exponential decay rate of $d=0.2$ applied at the end of each epoch. All weights are initialized with Xavier normal initializer. The batch size is fixed to $b=32$ for both models.
% both networks are trained using the Adam optimizer with the initial learning rate 10^{-3}, exponential decay rate of d=0.2 at the end of each epoch, all weights are initialized with Xavier normal initializer, batch size is fixed to b=32.
 %\subsection{Data Pre-processing} % TODO mention in initialization - the data pre-processing is the same as in the case of PBPE model
