\chapter{Proposed implementation}\label{chap:proposal}
In this chapter, we are going to describe the implementation we propose, the technologies we intend to use and the models we aim to examine.


\section{Technologies}
In this thesis, we implement all of the studied models in python deep learning library Keras with Tensorflow backend. All of the experiments are conducted on NVIDIA GTX 1070.

% ...Software, frameworks, libraries to be used... GPU used
\section{Benchmark datasets}

In this section, we are going to describe the datasets which will be used for the evaluation purposes of this thesis. All of the datasets mentioned below were made publicly available for research purposes.
\subsection{ITOP}
The Invariant-Top View Dataset (ITOP) \cite{haque2016viewpoint} consists of approximately 50k real-world depth images from two camera viewpoints (front view and top view). It captures 20 people, each performing 15 different actions. The dataset comes with the initial partition of the data into the train and test set. The train and test data contain around 18k and 5k samples from each viewpoint, respectively. Besides the depth images and the ground-truth joint labels, the dataset also includes raw point clouds. The skeletal model in this dataset is described by 15 body joints. Sample depth maps from the dataset are shown in Fig. \ref{fig:itop}. As can be seen in the figure, the depth images are rather noisy, but the noise can be partly reduced by several background segmentation methods.\par

\vspace{5mm}
\begin{figure}[H]
\begin{center}
  \includegraphics[height=130px]{images/implementation/itop.png}
  \caption[Sample depth images from the ITOP dataset \cite{haque2016viewpoint}.]{Sample depth images from the ITOP dataset \cite{haque2016viewpoint} (front and top view).}
  \label{fig:itop}
\end{center}
\end{figure}

\noindent
The current state-of-the-art on ITOP dataset was claimed in \cite{Marin18jvcir}, where the 100\% accuracy at 10cm precision was reached, which means every joint was predicted within 10cm from its ground-truth position. In the paper, they also claim to have reached the mean error of approximately 0.9cm on ITOP dataset (0.19cm on the front-view data), which is the best result achieved on the stated dataset, up to our knowledge.

\subsection{UBC3V}

The UBC3V \cite{Shafaei16} is a synthetic dataset made for the task of pose estimation from multiple cameras. It contains around 6 million synthetic depth frames structured in three parts according to the complexity of the human postures – easy, medium and hard pose. The pose in each frame is represented by the position of 18 skeletal joints. It captures a total of 16 characters and each frame is observed from three different viewpoints. Thus, the samples can be treated both as multi-view (point clouds from three cameras merged into one), or single-view postures (each sample handled separately). Although the multi-view point clouds describe a particular posture in a more complex way, considering the use-case in most of the real-time applications, the single-view samples are often the preferred option, due to the difficulty of the task of multiple camera synchronization.\par

\vspace{5mm}
\begin{figure}[H]
\begin{center}
  \includegraphics[height=130px]{images/implementation/ubc3v.png}
  \caption[Sample data from the UBC3V dataset \cite{Shafaei16}.]{Sample data from the UBC3V dataset (the figure shows the same pose from three different cameras) \cite{Shafaei16}.}
  \label{fig:ubc3v}
\end{center}
\end{figure}

\noindent
 The depth data are available in the form of depth images, but can be converted into point clouds in world reference coordinates using the intrinsic and extrinsic camera parameters. The ground truth labels are composed of 18 joints per posture, and the dataset also comprises a segmentation of each point cloud into 43 body regions (as can be seen in Fig \ref{fig:ubc3v}).

\subsection{MHAD}
The Berkeley Multimodal Human Action Database (MHAD) \cite{Vidal:2013:BMC:2478277.2478412} was recorded on real humans. It contains 11 actions performed by 7 male and 5 female subjects. Each subject performed each of the actions 5 times, which yields about 660 action sequences corresponding to about 82 minutes of total recording time. The total number of depth frames is over 250k. The skeleton structure in this dataset is defined by 35 joint locations. Two of the performed actions involve a chair, used for the subject to sit down and stand up. It is worth a mention, that the chair itself provides a lot of clutter in the depth data. The whole dataset was captured by multiple devices, including cameras, depth sensors, accelerometers and microphones. Two kinect cameras were used and synchronized to acquire the depth data, one placed in front of the subject, the other one at the back. Intrinsic and extrinsic camera parameters are provided to extract the scene as a point cloud with real-world coordinates.\par

\subsection{CMU Panoptic Dataset}
\cite{Joo_2017_TPAMI}

%\subsection{PKU-MMD}  % ? TODO  

%\subsection{Novel dataset} 
\section{Approaches}
The goal of our study is to implement several different approaches to the task of human pose estimation using neural networks. Among the methods we implement are both existing state-of-the-art neural models, as well as our novel approach we propose in this thesis – a two-stage pipeline called Four-channel Pose Estimation. We present the existing models we re-implement, and our novel approach in the subsequent sections.

\subsection{Deep Depth Pose model}
One of the aims of this work, is to implement the Deep Depth Pose model (DDP) \cite{Marin18jvcir} in Keras framework. We consider this an essential step to propose a state-of-the-art pose estimation model performing on depth data, since the DDP model claims to outperform all of the present methods on the examined datasets. Originally, the model is implemented in Matlab and while some parts of the code (mostly testing) are publicly available, the training procedures were not published.\par
\vspace{5mm}
\noindent
The main idea behind the stated method is incorporating a set of predefined prototype human poses. The convolutional neural network outputs a vector containing weights, each corresponding to particular prototype pose. The weighted prototypes are then summed up producing the final estimated pose. The predefined prototype poses should ideally cover the largest possible range of human poses, so that by their linear combination we can obtain any desired pose during the testing. Thus, the set of prototype poses is formed by applying K-means clustering on the training dataset.

\subsection{Point-Based Pose Estimation model}

% TODO toto cele uz do proposed implementation, potom v implementation uz implementacne detaily, preprocessing dat, upravy modelu
The Point-Based Pose Estimation model (PBPE) \cite{Ali19} is an approach to 3D human pose estimation from depth data, remarkable by several of its attributes. The whole network consists of two branches, the regression branch and the auxiliary segmentation branch. The output of the model is a 3D body pose, given by the coordinates of skeletal joints. One of the benefits of the method is the possibility of omitting the auxiliary branch in the testing phase, where the model is already trained, and by this restriction of the model's depth and complexity, the computational cost and prediction time can be cut down.\par
\vspace{5mm}
\noindent Of great significance for our thesis is the fact, that the input to the PBPE network is directly in a form of an unstructured point cloud. This may be convenient for a number of reasons, one of them being the acquisition of a sparser representation of the human body pose, in comparison to commonly used depth maps. The more density the input covers, the more computations have to be performed in the network, thus point clouds offer more effective data processing even in complex model architectures.

\subsection{Four-channel Pose Estimation}

An important part of our work is to develop a novel method for human pose estimation. We introduce the Four-channel Pose Estimation – a two-stage pipeline, which takes a point cloud as an input, and outputs the 3D coordinates of the estimated skeletal joint positions.
%\begin{minipage}{\textwidth}
%	Sily v našom modeli možno rozdeliť do dvoch skupín:
%	\begin{itemize}
%		\item interné sily
%		\item externé sily
%	\end{itemize}
%\end{minipage}\newline




