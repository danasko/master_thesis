\chapter{Overview}\label{chap:overview}

In this chapter, we will go through general overview of the techniques we are going to use in our solution. We will define basic terms associated with neural networks and their functioning, illustrate several scenarios on how neural networks can be utilized in image processing, and explain the concept of the pose estimation task, along with the various sub-categories we distinguish among.

\section{Neural networks}
  
In general, artificial neural networks were introduced as structures inspired by the biological structure of human brain. Similarly to the brain, the basic computational units of the artificial neural networks are neurons, which are connected together by synapses. In neural networks, synapses are simply weight values, which means the neuron performs some kind of calculation and the result is multiplied by the value corresponding to the particular connection it is passing through. The neurons are organized in layers, which is the main idea of the so-called deep learning. The neural networks can perform both supervised and unsupervised learning.\par
\vspace{5mm}
\noindent In most cases, we are dealing with the supervised learning, where we feed the model with the input data and also provide the output we expect the network to produce. As the input data is passed through the network in what we call a forward-pass, the neurons are outputting certain computed values and passing them to the neurons they are connected to, which in the end form a single or multiple outputs of the network. The idea of the supervised learning is that at the end of the forward-pass, we provide a feedback to the network, about how correctly it performed the desired task. This is done by comparing the actual output of the model to the desired output. The comparison is also referred to as calculating a loss function. The calculated loss is then back-propagated through all the layers in the network to adjust the weights of the model accordingly. This scenario is repeated in many iterations, while the loss value computed on the model output should be decreasing.\par
\vspace{5mm}
\noindent
Nowadays, as we have enough computational power and the amount of available data large enough to train on, the capacities and architectures of the neural networks are becoming increasingly extensive and powerful. 

\subsection{Image processing using deep learning}
Neural networks have shown their significant contribution to various research fields, and one of them is digital image processing. Many different tasks concerning the image processing can be carried out by a neural network, e.g. image classification, feature extraction, pattern recognition, object detection, image captioning etc. For instance, in image classification and feature extraction, deep learning has shown many advantages over the traditional methods, which rely on handcrafted features. Using a neural network model, instead of manually selecting and extracting image features, we can pass the image directly and the model learns to find and distinguish the important features itself, and additionally, might classify the image accordingly. While the first layers often recognize very simple features, like edges and corners, the deeper we dive into the network architecture, the more complex features are sought in particular layers and neurons within them.


\subsection{3D pose estimation}
One of the tasks concerning image processing and human motion analysis, frequently performed by neural networks, is the 3D pose estimation. As the name implies, it is a task of estimating the three-dimensional pose of a human subject from a single image or set of video frames. The resulting pose is determined by 3D coordinates of the skeletal joints of the human body. The number of joints can vary, thus it is usually considered a hyperparameter. The more skeletal joints (and output coordinates) the model estimates, the more complex representation of the human body skeleton we obtain.\par
\vspace{5mm}
\noindent
The human pose estimation approaches can be classified into model-based generative methods and discriminative methods. Generative methods treat the human body as an articulated structure. The utilized model generally describes the appearance of particular body parts and the spatial relationship between adjacent parts. Discriminative methods solve the task of pose estimation by direct regression from feature space to pose space, and are invariant to body shape variations. Deep learning approaches avoid the manual dealing with features and structural dependencies by embedding it into the mapping function and learning high-level representation of the input data.\par
\vspace{5mm}
\noindent
We mentioned the neural model infers the pose from an image, however, the actual form of the input data may vary. First, the individual approaches to the task can be divided in two categories according to the input data dimension. Two-dimensional input data are usually RGB images. The important advantage of such approaches is that they can be easily utilized in real-time in-the-wild applications, since a common RGB camera is sufficient to capture the images. This is also a reason, why a majority of the pose estimation research nowadays is focused on inferring from 2D data, as well as most of the public pose estimation datasets consist only of RGB images. On the other hand, three-dimensional input data bring an additional depth information into the neural networks, which is often very handful in increasing the accuracy of the estimation. The three-dimensional input data comes in various forms. The most common are depth maps. The basic idea behind depth maps is to encode the third dimension into the two-dimensional image, where at each pixel location, the corresponding pixel value is representing the value of the third coordinate (or depth) at the specified position. Other than depth maps, a convenient way to capture three-dimensional data is to use point clouds. Anyhow, point clouds have several attributes which are not suitable for neural networks. Most importantly, point clouds are unordered and irregular. Some of the researchers resolved this by making use of the voxelized grids, that is, discretized the point clouds into a predefined grid. Alternatively, several approaches evolved on the idea of processing the unorganized point clouds directly using shared multi-layer perceptrons to obtain the features of the point cloud and work further with them.\par


