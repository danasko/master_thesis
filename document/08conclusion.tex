\chapter{Conclusion}\label{chap:conclusion}
% na uvod vseobecny blabol, ofc
In this thesis, the task of 3D human pose estimation from depth data is studied, while the core of the research is focused on the impact of a particular form of input data on the estimation accuracy and computational efficiency.\par
\vspace{5mm}
\noindent
As the main contribution of our work, we propose a novel two-stage deep learning method for an accurate single-person depth-based human pose estimation called Segmentation-Guided Pose Estimation. We eliminate drawbacks related to the projection of 3D space to a 2D image, when estimating pose from depth maps, by introducing a concept of unordered point clouds as a permutation-invariant input to a neural network. To allow the network to maintain both local and global contextual information, we employ intermediate concatenation of extracted pointwise and aggregated features inside the model. Additionally, we perform semantic segmentation of the input point cloud into the corresponding body regions, and utilize the per-point region assignment as an extend of the input point cloud before the final regression. Our proposed approach proved to benefit from the redundancy of the input to the segmentation and regression network, being able to absorb the local and global context at once.\par
\vspace{5mm}
\noindent
We believe engaging sparse point clouds as an input to the neural network instead of the commonly used depth maps allows us to provide a representation of the human body that is easier to be perceived by the network, while lowering memory requirements and computational cost at the same time. Moreover, to help preserve gradient flow throughout the entire depth of the network, we improved the shared multi-layer perceptron modules by additional skip-connections. Our strategy achieves competitive results on a number of benchmark datasets, and outperforms state-of-the-art approaches.\par
\vspace{5mm}
\noindent
Aside from the benefits stated above, our interest to make use of the raw point clouds on the input, and focus on processing them directly, was aroused by the intention to apply the method proposed in this study on data obtained by the 3D MotionCam by Photoneo~\cite{photoneo}. The MotionCam is currently the highest resolution and highest accuracy 3D camera in the world. It produces a raw point cloud of the captured scene per frame. Thus, after being subsampled and normalized, the point cloud can be passed directly to our two-stage pose estimation pipeline.\par
\vspace{5mm}
\noindent
In addition to proposing a novel approach, a crucial step in our research process consisted of re-implementation of several existing state-of-the-art methods, mainly due to the absence of publicly released source code, and hence, the inability to reproduce the results and apply the methods on different data. We also present a few modifications of the existing strategies in order to increase the estimation accuracy.\par
\vspace{5mm}
\noindent
We consider an important part of this study to point out the most relevant limitations we encountered during the experiments. Regarding the depth-based human pose estimation, we see the biggest shortage in the range and accuracy of the available datasets. The suitable public datasets, containing both depth data of a captured human subject and the ground truth skeletal joint coordinates, are either too small to be used as training data for a neural network, or the accuracy of the ground truth labels is not sufficient. Moreover, even in large datasets, the data is often incomplete for certain sections, so the valid subset of the dataset ends up of a too small range after all. The limited accuracy of the ground truth poses is usually caused by poor synchronization of a depth sensor and a motion capture system. The most commonly used depth sensors do not have a stable frame rate, which results in time delays and misalignment between frames, and makes the precise synchronization practically impossible. In some of the datasets, this issue is partly fixed by time-stamping technique, refining the frame alignment, and filtering out the mismatches. It is even harder considering the multi-view approach, when the multiple depth sensors need to be synchronized mutually as well as with the motion capture system.\par
\vspace{5mm}
\noindent
As a potential future extend of this study, we suggest recording a large scale 3D pose estimation dataset using 3D camera and optical motion capture system simultaneously. Furthermore, the synthetically generated data could be utilized for a real data augmentation in the training process, what proved to be useful in several existing studies. As another direction of a follow-up research, the idea of processing raw point clouds might be extended to a level, where instead of processing the whole point cloud at once, the separate subsets of the point cloud are handled using the convolution kernels treated as non-linear density and weight functions. Such strategy might improve the accuracy in context of the pose estimation task, as well as it did in the existing methods performing semantic segmentation and classification tasks.\par